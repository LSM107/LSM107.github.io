---
layout: single

title:  "CS231n: What is Needed to Build Deep Neural Networks"

categories: Computer_Vision

tag: [CV]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True



---





이 포스팅은 '**CS231n의 Lecture 06~07**'에 대한 내용을 담고 있습니다.



자료 출처

- <https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf>









# Deep Neural Networks

이전 포스팅에서는 **Neural Networks가 무엇인지**, 어떻게 여러 층을 쌓아 만들어지는지, 그리고 이미지 데이터를 효과적으로 처리하기 위한 **CNN 구조**까지 살펴보았습니다. 그런데 그렇게 배운 내용들만으로 이전 포스팅에서 만들었던 모델은 전혀 학습이 되질 않았습니다. 혹시 모델의 용량이 부족했을까요? 그렇지 않습니다. 동일한 5층짜리 이미지 분류기만으로도 충분히 75% 이상의 정확도를 낼 수 있습니다. 문제는 용량이 아니라 학습의 방향에 있습니다.

현재 모델 내부의 파라미터들을 아무런 제약 없이 학습시키고 있는데, 이렇게 그냥 아무런 제한 없이 학습을 시켜버리면 파라미터들이 이상한 값들로 수렴해버려 모델이 망가져버리게 됩니다.

따라서 모델이 **더 안정적으로, 올바른 방향으로 학습될 수 있도록** 여러 가지 가이드라인을 설치할 필요가 있습니다. 구체적으로 어떤 문제가 발생하는지, 또 그 친구들을 어떻게 해결할 수 있는지 하나하나 살펴봅시다.







## Issue 01: Activation Function

<img src="/images/2025-08-17-CS231n_03/image-20250822151458033.png" alt="image-20250822151458033" style="zoom:50%;" />


$$
\sigma (x) = \frac{1}{1+e^{-x}}
$$

우리가 그동안 사용했던 Sigmiod 함수는 정말 인기가 많**았**습니다. 왜냐하면 이 함수의 모양이 뉴런을 표현할 때 좋은 해석을 제공했기 때문인데요, 보통 사람의 뉴런을 표현할 때 0 아니면 1로 이진 발화한다고 모델링하는 경우가 많거든요. 그러니까 뉴런이 자극을 받으면 그 다음 뉴런으로 자극을 전달하는 값이 있거나(1) 없거나(0) 둘 중 하나라고 모델링합니다. 그리고 또 결정론적으로 발화한다고 보지 않고 확률적으로(자극을 받은 경우에도 어떤 때에는 자극을 전달하지 않는 경우가 있을 수 있다는 거) 발화한다고 보기 때문에 0과 1 사이를 부드럽게 이어주는 이 함수를 뉴런이 발화할 확률이라고 사용하기 딱이었던 거죠. 그런데 그런 식의 해석의 용이성이 있다 뿐, 너무나 많은 문제를 가지고 있습니다.



가장 첫번째로 **모델이 깊어지면 깊어질수록 초반 레이어들에 전해지는 기울기가 급격하게 감소합니다.** 3층짜리 인공 신경망 예시에서 그 구체적인 상황을 살펴봅시다.



$$
z^{[1]} = W^{[1]}x + b^{[1]}
$$

$$
a^{[1]} = \sigma(z^{[1]})
$$


$$
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}
$$

$$
a^{[2]} = \sigma(z^{[2]})
$$


$$
z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}
$$

$$
\hat y = \sigma(z^{[3]})
$$


$$
L = \text{MSE}(y, \hat y)
$$



위 모델에서 첫 번째 층, 두 번째 층, 세 번째 층 파리미터에 대한 기울기를 구하기 위한 Chain Rule 식을 써보면 아래와 같습니다.



$$
\frac{\partial L}{\partial W^{[3]}}
= \frac{\partial L}{\partial \hat y}
\cdot 
\underbrace{\frac{\partial \hat y}{\partial z^{[3]}}}_{\text{sigmoid}}
\cdot \frac{\partial z^{[3]}}{\partial W^{[3]}}.
$$

$$
\frac{\partial L}{\partial W^{[2]}} 
= \frac{\partial L}{\partial \hat y} 
\cdot 

\underbrace
{\frac{\partial \hat y}{\partial z^{[3]}}}
_{\text{sigmoid}}

\cdot \frac{\partial z^{[3]}}{\partial a^{[2]}} 
\cdot 

\underbrace
{\frac{\partial a^{[2]}}{\partial z^{[2]}}}
_{\text{sigmoid}}

\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}
$$

$$
\frac{\partial L}{\partial W^{[1]}} 
= \frac{\partial L}{\partial \hat y} 
\cdot 

\underbrace
{\frac{\partial \hat y}{\partial z^{[3]}}}
_{\text{sigmoid}}

\cdot \frac{\partial z^{[3]}}{\partial a^{[2]}} 
\cdot 

\underbrace
{\frac{\partial a^{[2]}}{\partial z^{[2]}}}
_{\text{sigmoid}}

\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}
\cdot 
\underbrace
{\frac{\partial a^{[1]}}{\partial z^{[1]}}}
_{\text{sigmoid}}
\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}
$$







초반 레이어로 가면 갈수록 이후 레이어들의 각 요소들에 대한 기울기가 계속 쌓여가는 것을 확인할 수 있죠. 바로 이 지점에서 문제가 발생합니다. 두 변수가 Sigmoid로 이어진 부분에 아래 중괄호로 표시를 해두었는데, 출력 지점을 기준으로 레이어를 거듭할수록 Sigmoid에 의한 기울기 요소가 1개씩 추가되죠. 문제는..



<img src="/images/2025-08-17-CS231n_03/image-20250822160508222.png" alt="image-20250822160508222" style="zoom:50%;" />

Sigmoid 함수의 모양입니다. Sigmoid 함수는 0일 때 기울기가 0.25로 가장 크고 입력의 절댓값이 5을 넘어서게 되면 기울기가 거의 0에 수렴합니다. 그러니까 $\frac{\partial a^{[.]}}{\partial z^{[.]}}$이 0과 0.25 사이의 어떤 값으로 구해지는데, 초반 레이어로 가면 갈수록 이 값이 Chain Rule에서 더 많이 곱해지므로 최종적으로는 초반 레이어의 파라미터들은 손실에 대한 기울기가 0.000... 의 아주 작은 값으로 나타납니다. 그 말인 즉, 초반 레이어에 있는 파라미터들을 당장 조금 움직여봤자, 전체 손실에는 전혀 변화를 주지 못한다는 말이 됩니다. Sigmoid를 사용하는 모델에서는 초반 레이어의 파라미터들이 예측 결과에 아무 영향도 주지 못하는 중요하지 않은 부분으로 전락해버린다는 의미일까요?



Sigmoid 실수 전 범위를 입력받아 0과 1 사이의 값을 출력하며, 특히 입력이 0 근처일 때 출력이 빠르게 변합니다. 이 함수를 층층이 반복해서 적용하면(여러 층의 Sigmoid를 쌓는 경우), 각 층의 포화가 누적되어 입력–출력 관계가 점점 **완만한 S-곡선에서 계단(step) 함수처럼 급격히 넘어가는 형태**로 바뀝니다. 이로 인해 피크(값이 급격하게 변하는 지점)를 제외한 넓은 **plateau 구간**에서는 기울기가 거의 0에 가까워져 **파라미터가 거의 업데이트되지 않는** 문제가 발생하게 됩니다. 구체적인 예시를 들어서 살펴봅시다.



<img src="/images/2025-08-17-CS231n_03/image-20250823145828484.png" alt="image-20250823145828484" style="zoom:40%;" />

2차원 공간에 두 개의 군집 데이터를 생성하고, 이를 분류하기 위한 간단한 신경망을 구성합니다. 모든 레이어는 **뉴런 2개짜리 FC-Layer**으로 이루어지며, **활성화 함수는 Sigmoid**를 사용합니다. 이제 **2층 네트워크**와 **9층 네트워크**를 각각 간단히 학습시킨 뒤, 학습된 상태에서 **첫 번째 레이어의 두 뉴런에서 각각 선택한 가중치 하나($w_{0, 0}$, $w_{1, 0}$)**를 축으로 하고, 나머지 파라미터는 고정한 채 두 파라미터에 대해서만 손실이 어떻게 변하는지 평가해보면요..



<img src="/images/2025-08-17-CS231n_03/image-20250823150708899.png" alt="image-20250823150708899" style="zoom:40%;" />

<img src="/images/2025-08-17-CS231n_03/image-20250823150733970.png" alt="image-20250823150733970" style="zoom:40%;" />

그 결과 얻은 손실 평면을 보면, 아주 극적인 차이는 아니지만 깊은 네트워크에서 손실 값이 바뀌는 구간이 더 날카롭게 드러나는 경향이 있는 것으로 보입니다. 이것을 기울기로 보면 차이가 더욱 분명해지는데요..



<img src="/images/2025-08-17-CS231n_03/image-20250823151107817.png" alt="image-20250823151107817" style="zoom:40%;" />

<img src="/images/2025-08-17-CS231n_03/image-20250823151055751.png" alt="image-20250823151055751" style="zoom:40%;" />

깊은 신경망에서는 첫 번째 레이어의 파라미터에 대한 기울기 피크 영역의 크기가 감소하지만, 피크의 세기는 커지는 것을 확인할 수 있습니다. 대부분의 영역은 평탄(plateau)해서 조금 움직여도 손실이 거의 변하지 않고, 아주 좁은 전이대에 들어갈 때만 손실이 급격히 변하게 되죠.

**첫 번째 레이어의 파라미터들이 신경망에서 중요하지 않은 것이 아닙니다.** 다만 신경망이 깊어질수록 손실이 변하는 구간이 좁아지고, 그 이외의 영역은 평탄해지기 때문에 작은 업데이트로는 손실의 변화를 일으키지 못하게 되는 문제 상황이 발생합니다. 
































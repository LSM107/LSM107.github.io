---
layout: single

title:  "CS231n: What is Needed to Build Deep Neural Networks"

categories: Computer_Vision

tag: [CV]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True



---





이 포스팅은 '**CS231n의 Lecture 06~07**'에 대한 내용을 담고 있습니다.



자료 출처

- <https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf>









# Deep Neural Networks

이전 포스팅에서는 **Neural Networks가 무엇인지**, 어떻게 여러 층을 쌓아 만들어지는지, 그리고 이미지 데이터를 효과적으로 처리하기 위한 **CNN 구조**까지 살펴보았습니다. 그런데 그렇게 배운 내용들만으로 만든 모델은 제대로 학습이 되지 않았습니다. 혹시 모델의 표현 용량이 부족했던 걸까요? 아닙니다. 우리가 배운 구조만으로도 충분히 큰 모델을 만들 수 있습니다.

문제의 핵심은 **학습이 올바른 방향으로 진행되지 않는다**는 점입니다. 지금은 모델 내부의 파라미터들을 아무런 제약 없이 학습시키고 있는데, 이렇게 그냥 아무런 제한 없이 학습을 시켜버리면 파라미터들이 이상한 값들로 수렴해 버려 모델이 망가져버리게 됩니다.

따라서 모델이 **더 안정적으로, 올바른 방향으로 학습될 수 있도록** 여러 가지 가이드라인을 설치할 필요가 있습니다. 구체적으로 어떤 문제가 발생하는지, 또 그 친구들을 어떻게 해결할 수 있는지 하나하나 살펴봅시다.
























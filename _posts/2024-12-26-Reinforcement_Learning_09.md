---
layout: single

title:  "강화학습 09: 근사를 이용한 활성 정책 예측"

categories: Reinforcement Learning

tag: [Reinforcement Learning, TD]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: false
---



**글에 들어가기 앞서...**

이 포스팅은 '**강화학습**'에 대한 내용을 담고 있습니다.



자료 출처: 단단한 강화학습, Reinforcement Learning An Introduction , 2nd edition. 리처드 서튼, 앤드류 바트로, 김성우(옮긴이)









# 근사를 이용한 활성 정책 예측

여기서부터 본격적으로 근사를 통해 정책을 예측하는, 가치를 추정하는 방법론에 대해서 다루게 됩니다. 이전 포스팅의 알파고 제로 알고리즘에 이미 근사를 이용한 활성 정책을 사용합니다. 바둑은 분명 매우 복잡하고 고수의 반열에 오르기까지 굉장한 노력을 필요로하는 게임임은 분명하지만, 사실 우리가 실제로 다루는 문제들, 예를 들면, 운전을 하는 에이전트라든가, 축구라든가, 이런 게임들과는 비교할 수 없을 정도로 단순한 문제입니다. 여기에서 단순하다는 말은 전문가 수준에 이르기까지 필요로하는 노력의 수준이 아닌, 가능한 상태의 가짓수를 지칭하는 표현입니다. 바둑은 게임의 규칙상 이산적인 상태를 가지기 때문에 상태의 종류가 한정적입니다. 그럼에도 불구하고 바둑은 표를 이용해서 풀 수 없을만큼 많은 상태공간을 가지는데, 연속적인 상태 공간을 가지는 문제를 표를 이용해서 풀기란 불가능합니다. 그런데 우리에겐 이런 문제점들을 해결할 수 있는 강력한 도구가 있는데요, 그것은 바로 **인공 신경망**입니다.




































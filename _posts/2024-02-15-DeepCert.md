---
layout: single

title:  "DeepCert: Verification of Contextually Relevant Robustness for Neural Network Image Classifiers"

categories: Paper

tag: [verification, robustness, neural network, adversarial attack]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
---



#### 글에 들어가기 앞서..

이 포스팅은 '***DeepCert: Verification of Contextually Relevant Robustness for Neural Network Image Classifiers***' 논문에 대한 소개를 담고 있습니다.



논문 원본은 아래의 링크를 통해 확인할 수 있습니다.

- <https://theory.stanford.edu/~barrett/pubs/PWG+21.pdf>





## 배경

신경망을 기반으로 한 이미지 분류 모델의 정확도는 근 몇년간 굉장히 빠른 속도로 상승했고, 심지어는 일부 벤치마크에서는 인간 수준의 정확도를 뛰어넘는 성능을 보이기도 합니다. 이렇듯 신경망은 정말 강력한 도구이지만, **적대적 공격(adversarial attack)**에 있어 취약한 모습을 보입니다.



![image-20240215230659921](/images/2024-02-15-DeepCert/image-20240215230659921.png)

- 그림 출처: <https://pytorch.org/tutorials/beginner/fgsm_tutorial.html>

위의 사례에서 신경망은 사람 눈에는 분명 같아보이는 두 이미지를 다른 클래스로 분류합니다. 이런 일이 발생하는 이유는, 사람이 인식하기 힘든 수준의 정말 작은 perturbation을 이미지에 가해졌기 때문입니다. 그리고 이미지에 가해진 perturbation은 속이고자 하는 특정 신경망을 목표로 적대적으로 결정됩니다. 위와 같은 perturbation이 가해져 신경망을 속이는 예제를 **적대적 예제(adversarial example)**라고 부릅니다.



특정 신경망의 강건성을 적대적 예제를 통해 판단할 수 있습니다. perturbation에 범위가 존재하지 않는다면, perturbation이 가해진 이미지는 어떤 이미지도 될 수 있기 때문에 적대적 예제는 무조건 존재합니다. 따라서 적대적 예제를 이용해서 강건성을 평가할 때, 아래와 같은 기준들을 사용합니다.

1. $0<\epsilon < p$​ 의 범위에서 적대적 예제가 존재하지 않는다면, 모델은 충분히 강건하다.
   - $\epsilon$은 모델에 가해지는 perturbation의 크기입니다.
2. 모델에 대한 적대적 예제들 중, perturbation이 가장 작은 적대적 예제의 그 크기를 통해 모델의 강건성을 평가한다.



적대적 예제에 가해지는 perturbation의 크기는 대개 $L_p-norm$ 으로 측정됩니다. 때문에 적대적 예제는 다소 pixel-wise한 perturbation이 가해져 생성됩니다. 이런 방식으로 생성된 적대적 예제는 ACAS Xu 벤치마크와 같은 상황에서는 의미있는 강건성 검증의 기준이 될 수 있습니다. **하지만 이런 방식으로 생성되는 적대적 예제는 현실 세계에서 발생하는 노이즈를 반영하기 힘들 수 있습니다**.



![img](/images/2024-02-15-DeepCert/1*1FMKI3BuS-ZQFvF4FElxIQ.png)

- 그림 출처: <https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9>

위의 사진은 데이터 증강을 위해 인위적으로 만들어진 예제이지만, 현실 세계에서도 충분히 있을 법 합니다(물체가 카메라를 통해 다양한 각도로 찍힐 수 있기 때문). 특히, 위의 사례에서는 회전 각도가 물체의 클래스를 분류하는데 있어 영향을 주지 않아야 할 것입니다. 하지만,  $L_p-norm$​으로 원본 이미지와의 거리를 측정하면, 그 값이 상당히 크게 나타납니다. 이미지 전체가 이동하는 방식으로 perturbation이 가해지기 때문입니다. **따라서 전통적인 적대적 예제를 통한 강건성 검증은 회전 perturbation에 대한 강건성을 보장할 수 없습니다. 이런 문제는 회전 뿐만 아니라 안개 효과(hazing), 대조 효과(contrast), 흐림 효과(blurring)에서도 마찬가지로 나타납니다.**



따라서 이 논문에서는 현실 세계에서 발생하는 노이즈에 대한 강건성을 의미하는 **contextually relevant robustness**에 대한 검증 기법을 제안합니다.





## 핵심 아이디어




---
layout: single

title:  "CS231n: Backpropagation & NN(Neural Networks)"

categories: Computer_Vision

tag: [CV]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True


---





이 포스팅은 '**CS231n의 Lecture 04~07**'에 대한 내용을 담고 있습니다.



자료 출처

- <https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf>









# Neural Networks

지난 포스팅에 이어 Gradient Descent에 대해 좀 더 다뤄보고, 이어서 선형 분류기보다 더 진보한 방식이라 말할 수 있는 인공 신경망에 대해서 알아봅니다.







## Gradient Descent

![GD](/images/2025-07-21-CS231n_02/GD.gif)

앞선 포스팅에서 다룬 Gradient Descent는 손실함수의 값을 낮출 때 사용되는 정말 강력한 도구입니다. 용어 그대로 Gradient를 따라 파라미터 값을 바꾸면서 점점 낮은 손실 값을 찾아가는 방법이기 때문에 파라미터와 손실 값 사이의 Gradient를 알고 있어야 사용할 수 있습니다.

Gradient를 구하는 방식에 따라 **Numerical Gradient**와 **Analytic Gradient**로 구분해 부릅니다. 


$$
\frac{df(x)}{dx} = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
$$


**Numerical Gradient**는 실제로 파라미터에 미소 변화를 주었을 때 손실 값의 변화를 측정하고 이들을 나누어 기울기를 계산합니다. 우리가 고등학교 때 배운 위의 미분 정의를 나타내는 식을 직접적으로 사용하는 것이죠. 이때 $h$는 무한히 작은 값일수록 더 정확해지지만 현실적으로는 적당히 아주 작은 숫자를 사용해 기울기를 구하게 됩니다. 그런데 이 방법을 통해 기울기를 구하려면 각 파라미터를 조금씩 바꿀 때마다 바뀐 손실함수 값을 알아야 하거든요? 그 말은 파라미터 개수만큼 evaluation을 수행해야 한다는 말이랑 같은데요, 당연히 시간이 엄청나게 많이 소요됩니다. 심지어는 그렇게 구한 기울기를 그리 정확하지도 않습니다. 이러한 문제점들로 인해 Numerical Gradient는 직관적이긴 하지만 잘 사용되지 않고 Analytic Gradient의 디버깅 용도로만 종종 사용됩니다.

**Analytic Gradient**는 각 변수에 대한 편미분 식을 직접 구합니다. 마치 고등학교에서 어떤 식에 대해서 미분을 할 때 도함수를 계산하잖아요? 그거랑 동일합니다. 그렇게 구한 편미분 기울기는 원리적으로 정확한 값입니다. 그런데 이걸로 구하려면 모든 파라미터에 대해서 편미분을 해야한다는 얘기 같아서 너무 복잡할 것 같은데, 실제로는 바로 아래에서 설명할 Backpropagation 기법을 통해 단계적으로, 그리고 효율적으로 구해집니다.





### Backpropagation(간단한 예제)


$$
f(x, y, z) = (ax + by) \times cz
$$


위의 모델을 가정해봅시다. 이 식에서 $x$, $y$, $z$는 입력 값이구요, $a$, $b$, $c$가 우리가 모델에서 조정할 수 있는 파라미터입니다. 손실함수는 단순하게 정답과 모델의 예측 값 사이의 차로 정의하겠습니다. 처음에 파라미터를 아래와 같이 랜덤으로 설정해보고, 모델을 평가하기 위한 하나의 훈련 데이터를 생각해볼게요.


$$
f(x, y, z) = (2x + 5y) \times 1z
$$

$$
\text{train example:}((x=1, y=2, z=1), \text{ground truth}=15)
$$



훈련 데이터를 모델에 넣어보면 예측 값이 12이 나오고, 실제 Ground Truth 값은 15니까 손실 값은 3가 됩니다. Gradient Descent를 통해 업데이트하기 위해서는 $a$, $b$, $c$와 모델의 손실 값 사이의 기울기를 알아야 합니다.


$$
\frac{\partial L}{\partial a}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial c}
$$


세 개의 파라미터의 기울기가 Gradient Descent를 할 때 필요한 것들이고, 이 친구들은 Forward Propagation과 Backprogation을 통해 구해집니다.



![BP1](/images/2025-07-21-CS231n_02/BP1.gif)

먼저 **Forward Propagation**을 수행하는데, 예측 값을 구하기 위해 모델의 구조에 따라 순차적으로 계산해 나아가는 과정을 의미합니다. 그 과정 사이사이의 중간 계산 결과를  중간변수로 모두 저장해둡니다. 이어서 바로 **Backpropagation**을 실행합니다. Backpropagation의 핵심은 Chain Rule을 통한 기울기 전파입니다. 파라미터와 손실 값 사이의 기울기를 바로 계산하기는 복잡하지만, 중간변수 사이의 기울기를 구하는 과정은 굉장히 간단합니다(더하기와 곱하기 단위로 중간변수를 지정할 것이기 때문에). 그리고  매개변수와 손실 값 사이의 있는 중간변수들간의 기울기들을 Chain Rule을 통해 모조리 다 곱하면 결과적으로 손실 값과 파라미터 사이의 기울기를 구할 수 있게 됩니다. 

딱 봐도 동적 프로그래밍으로 구현하기 쉬운 구조로 보입니다. 실제로도 동적 프로그래밍을 사용해 굉장히 효율적으로 모든 파라미터에 대한 기울기를 구하게 됩니다.





### Backpropagation(행렬연산)

방금까지 정말 간단한 모델에서 Backpropagation을 살펴봤습니다. 그런데 저렇게까지 간단한 모델을 사용하는 경우는 없구요, 대개는 파라미터가 행렬로 존재해서 많은 계산이 한 번에 수행되는 형태를 가집니다. 


$$
f(x) = ||W \cdot x||^2
$$


위와 같은 모델을 생각해봅시다. 행렬이 결국에는 여러 변수의 계산을 간단하게 처리하기 위해서 묶어놓은 것일 뿐이잖아요? 그렇기 때문에 형태만 조금 복잡할 뿐,Backpropagation에서 사용되는 핵심 아이디어는 동일합니다. 아래에 예시로 든 모델의 파라미터 기울기를 구하는 과정을 나타내 보았으니 참고해보세요.

![mgd](/images/2025-07-21-CS231n_02/mgd.gif)







## Neural Networks

지금까지 선형 분류 모델 구조, 그리고 해당 모델을 최적화시키기 위한 여러 기법들에 대해서 다루어 봤습니다. 그런데요 선형 분류 모델으로는 죽었다 깨어나도 절대 풀지 못하는 문제들이 있습니다. 가장 대표적으로는 **XOR** 분류 문제가 있습니다.



```python
import numpy as np
import matplotlib.pyplot as plt

def xor(a, b):
    """
    Compute the XOR operation for two binary inputs.
    Args:
        a (int): First binary input (0 or 1).
        b (int): Second binary input (0 or 1).
    Returns:
        int: Result of XOR operation (0 or 1).
    """
    return np.bitwise_xor(a, b)

# Example usage
inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
for a, b in inputs:
    print(f"XOR({a}, {b}) = {xor(a, b)}")

# XOR data
def generate_xor_data():
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XOR labels
    return X, y

# Linear classifier
class LinearClassifier:
    def __init__(self, learning_rate=0.1, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def forward(self, X):
        """
        Compute the linear output.
        """
        return np.dot(X, self.weights) + self.bias

    def train(self, X, y):
        """
        Train the linear classifier using gradient descent.
        """
        n_samples, n_features = X.shape
        self.weights = np.random.randn(n_features)
        self.bias = 0

        for _ in range(self.epochs):
            # Compute predictions
            linear_output = self.forward(X)
            predictions = 1 / (1 + np.exp(-linear_output))  # Sigmoid activation

            # Compute gradients
            errors = predictions - y
            grad_weights = np.dot(X.T, errors) / n_samples
            grad_bias = np.mean(errors)

            # Update parameters
            self.weights -= self.learning_rate * grad_weights
            self.bias -= self.learning_rate * grad_bias

    def predict(self, X):
        """
        Predict binary labels for input data.
        """
        linear_output = self.forward(X)
        predictions = 1 / (1 + np.exp(-linear_output))  # Sigmoid activation
        return (predictions >= 0.5).astype(int)

# Visualization
def visualize_decision_boundary(classifier, X, y):
    """
    Visualize the decision boundary of the trained classifier.
    """
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = classifier.predict(grid).reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.title("Decision Boundary of Linear Classifier on XOR Data")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Main execution
X, y = generate_xor_data()
classifier = LinearClassifier(learning_rate=0.1, epochs=1000)
classifier.train(X, y)
visualize_decision_boundary(classifier, X, y)
```



<img src="/images/2025-07-21-CS231n_02/image-20250728230415643.png" alt="image-20250728230415643" style="zoom:50%;" />

XOR 문제는 말 그대로 XOR로 위치하는 두 집단을 분류하는 문제입니다. 그리고 보시는 바와 같이, 그 두 집단을 하나의 직선으로 분류하는 일은 원리적으로 불가능합니다. 그렇다면 직선을 여러 개를 사용하거나 곡선을 사용하면 이런 문제들을 해결할 수 있을 것 같다는 생각이 듭니다. 그걸 해주는게 **Neural Networks** 입니다.

해결책으로 선형 레이어를 여러 개를 쌓으면 되지 않을까? 하는 생각이 듭니다. 그런데요 선형 계층을 무수히 많이 쌓아봤자 수학적으로는 하나의 선형 계측과 전혀 다르지 않습니다.


$$
\mathbf{h} = W_{1}\,\mathbf{x} + \mathbf{b}_{1}
$$

$$
\mathbf{y} = W_{2}\,\mathbf{h} + \mathbf{b}_{2}
$$



위 수식은 두 선형 계층이 $\mathbf h$ 라는 매개변수를 통해 이어진 모델을 나타냅니다. 두 식을 하나의 식으로 합쳐서 표현해볼게요.


$$
\mathbf{y}
= \bigl(W_{2} W_{1}\bigr)\,\mathbf{x}
  + \bigl(W_{2}\mathbf{b}_{1} + \mathbf{b}_{2}\bigr),
$$

$$
\mathbf{y}= W\,\mathbf{x} + \mathbf{b}
$$



두 수식을 합쳤더니 하나의 선형 계층으로 축약되어 버리는 결과를 확인할 수 있습니다. 이게 선형 계층을 아무리 많이 쌓아도 하나의 선형 계층과 전혀 다르지 않은 이유입니다. 이런 문제점을 해결하기 위해서 다음 선형 계층으로 넘어가기 전에 비선형 함수인 **Activation Function**을 넣어 줍니다. 


$$
\mathbf{z} = W_{1}\,\mathbf{x} + \mathbf{b}_{1}
$$

$$
\mathbf h = \phi(\mathbf z)
$$

$$
\mathbf{y} = W_{2}\,\mathbf{h} + \mathbf{b}_{2}
$$



이렇게 Activation Function을 사용해 선형 변환을 거친 출력을 휘어주면 여러 계층이 하나의 선형 함수로 더 이상 통합되지 못하고 각각이 결정 평면을 조절하는 역할을 수행하게 됩니다. 결과적으로는 복잡하게 구부러진 결정 평면을 만들 수 있게 되는 것입니다. 


$$
\mathbf{z}_1 = W_1\mathbf{x} + \mathbf{b}
$$

$$
\mathbf{h}_1 = \phi(\mathbf{z}_1)
$$



**Perceptron**은 '입력을 받아 선형 결합을 하고, 그 결과를 다시 비선형 함수로 가공해내는 최소 단위'를 의미합니다. 위에서는 행렬을 사용해서 한 번에 계산이 되기는 했지만, 사실 행렬을 통한 연산은 여러 개의 벡터 내적 연산을 함께 수행하는 것일 뿐이잖아요? 그 벡터 내적 연산이 되는 요소 하나하나를 Perceptron이라고 부릅니다. 예를 들어 입력 차원이 4차원이고 가중치 벡터가 3 by 4라면, 4차원 벡터끼리의 내적이 3번 수행되고 그 각각에 bias를 더한 다음 Activation Function을 가해 얻은 결과가 3차원 결과 벡터라는 거죠. 즉 3개의 Perceptron을 사용해 4차원 입력이 3차원 출력을 얻게 되었다고 말하는 것과 동일합니다. Perceptron은 기본적으로 모든 입력 값을 연산의 재료로 사용하기 때문에 Perceptron만으로 구성된 계층을 **FC(Fully-Connected)** Layer라고 부릅니다.



```python
import numpy as np
import matplotlib.pyplot as plt

# XOR data with additional slight noise for robustness
def generate_xor_data(noise=0.0):
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)
    if noise > 0:
        X += np.random.normal(scale=noise, size=X.shape)
    y = np.array([0, 1, 1, 0])  # XOR labels
    return X, y

# 2-layer neural network with Xavier initialization and Adam optimizer
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(a):
    return a * (1 - a)

class TwoLayerNN:
    def __init__(self, hidden_size=4, learning_rate=0.01, epochs=20000,
                 beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.hidden_size = hidden_size
        self.lr = learning_rate
        self.epochs = epochs
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

    def initialize_parameters(self, input_size):
        # Xavier/Glorot initialization
        limit_in = np.sqrt(6 / (input_size + self.hidden_size))
        self.W1 = np.random.uniform(-limit_in, limit_in, (input_size, self.hidden_size))
        self.b1 = np.zeros((1, self.hidden_size))
        limit_out = np.sqrt(6 / (self.hidden_size + 1))
        self.W2 = np.random.uniform(-limit_out, limit_out, (self.hidden_size, 1))
        self.b2 = np.zeros((1, 1))

        # Adam moment vectors
        self.mW1 = np.zeros_like(self.W1)
        self.vW1 = np.zeros_like(self.W1)
        self.mb1 = np.zeros_like(self.b1)
        self.vb1 = np.zeros_like(self.b1)
        self.mW2 = np.zeros_like(self.W2)
        self.vW2 = np.zeros_like(self.W2)
        self.mb2 = np.zeros_like(self.b2)
        self.vb2 = np.zeros_like(self.b2)

    def forward(self, X):
        self.X = X
        self.z1 = X @ self.W1 + self.b1
        self.a1 = np.tanh(self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = sigmoid(self.z2)
        return self.a2

    def compute_gradients(self, y):
        m = y.shape[0]
        y = y.reshape(-1, 1)
        # Output layer gradient for cross-entropy+sigmoid
        dz2 = (self.a2 - y) / m
        dW2 = self.a1.T @ dz2
        db2 = np.sum(dz2, axis=0, keepdims=True)
        # Hidden layer gradient
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (1 - np.tanh(self.z1)**2)
        dW1 = self.X.T @ dz1
        db1 = np.sum(dz1, axis=0, keepdims=True)
        return dW1, db1, dW2, db2

    def update_parameters_adam(self, dW1, db1, dW2, db2, t):
        # Update biased first moment estimate
        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dW1
        self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * db1
        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dW2
        self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * db2
        # Update biased second moment estimate
        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (dW1**2)
        self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (db1**2)
        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (dW2**2)
        self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (db2**2)
        # Compute bias-corrected moments
        mW1_hat = self.mW1 / (1 - self.beta1**t)
        mb1_hat = self.mb1 / (1 - self.beta1**t)
        mW2_hat = self.mW2 / (1 - self.beta1**t)
        mb2_hat = self.mb2 / (1 - self.beta1**t)
        vW1_hat = self.vW1 / (1 - self.beta2**t)
        vb1_hat = self.vb1 / (1 - self.beta2**t)
        vW2_hat = self.vW2 / (1 - self.beta2**t)
        vb2_hat = self.vb2 / (1 - self.beta2**t)
        # Update parameters
        self.W1 -= self.lr * mW1_hat / (np.sqrt(vW1_hat) + self.epsilon)
        self.b1 -= self.lr * mb1_hat / (np.sqrt(vb1_hat) + self.epsilon)
        self.W2 -= self.lr * mW2_hat / (np.sqrt(vW2_hat) + self.epsilon)
        self.b2 -= self.lr * mb2_hat / (np.sqrt(vb2_hat) + self.epsilon)

    def train(self, X, y):
        self.initialize_parameters(X.shape[1])
        for epoch in range(1, self.epochs + 1):
            output = self.forward(X)
            # Loss calculation (binary cross-entropy)
            eps = 1e-8
            loss = -np.mean(y.reshape(-1,1) * np.log(output + eps) + (1 - y.reshape(-1,1)) * np.log(1 - output + eps))
            # Gradients
            dW1, db1, dW2, db2 = self.compute_gradients(y)
            # Adam update
            self.update_parameters_adam(dW1, db1, dW2, db2, epoch)
            # Logging
            if epoch % 1000 == 0:
                print(f"Epoch {epoch:5d} — loss: {loss:.4f}")

    def predict(self, X):
        return (self.forward(X) >= 0.5).astype(int)

# Test cases
def test_xor():
    X, y = generate_xor_data(noise=0.0)
    model = TwoLayerNN(hidden_size=8, learning_rate=0.01, epochs=5000)
    model.train(X, y)
    preds = model.predict(X).flatten()
    print("XOR predictions:", preds)
    assert np.array_equal(preds, y), "Model failed to learn XOR"

if __name__ == "__main__":
    test_xor()
    print("All tests passed!")

# Visualization
X, y = generate_xor_data(noise=0.02)
model = TwoLayerNN(hidden_size=8, learning_rate=0.01, epochs=5000)
model.train(X, y)

# Plot decision boundary
def visualize_decision_boundary(classifier, X, y):
    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1
    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = classifier.predict(grid).reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)
    plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.title("Decision Boundary on XOR with Xavier + Adam")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

visualize_decision_boundary(model, X, y)

```

이 코드가 퍼셉트론을 2층으로 쌓아 만든 모델이구요, 아래 그림이 모델 학습 결과로 얻은 결과입니다.



<img src="/images/2025-07-21-CS231n_02/image-20250728234117419.png" alt="image-20250728234117419" style="zoom:50%;" />





### Convolution Neural Networks

일반적으로 MLP로 이미지를 분류하려면 일단 2차원 이미지를 일렬로 펴서 하나의 벡터로 만들어야 합니다. Perceptron은 모든 입력값과 연결되기 때문에 시작 정보의 공간 근접성이라는 특징을 무시합니다. 그러니까 굳이 볼 필요 없어 보이는 픽셀들을 함께 묶어 연산도 포함되는데, 이게 꽤나 많은 부분을 차지해서 특히 고해상도 이미지에서 크게 비효율적입니다. **CNN**(Convolution Neural Networks)는 주변 픽셀을 보자는 아이디어로 공간적 근접성을 고려할 수 있는 커널이라는 아이디어를 도입해 파라미터의 개수를 획기적으로 줄이면서도 더 높은 성능을 보여줍니다.



![cf](/images/2025-07-21-CS231n_02/cf.gif)



위와 같이 7  by 7 이미지가 있으면 저렇게 3 by 3의 Filter를 이동시키면서 겹치는 부분의 픽셀들을 모두 곱해 하나의 스칼라 값을 반환합니다. 위 그림에서는 가장 윗줄에 대해서만 이동시키면서 계산을 하고 있는데 다른 줄에 대해서도 동일한 연산을 수행하게 됩니다. 필터는 이미지를 벗어나지는 않기 때문에 CNN 연산을 수행하게 되면 원래 이미지보다 같거나 작은 크기의 출력을 얻게 됩니다. 위 예시에는 5 by 5의 이미지를 출력으로 얻게 됩니다.



![cf2](/images/2025-07-21-CS231n_02/cf2.gif)

필터를 움직이는 보폭은 반드시 한 칸으로 정해진 것은 아니어서 이렇게 더 널찍이 이동할 수도 있습니다. 이 경우 출력의 크기가 당연히 더 줄어들게 되겠죠.



<img src="/images/2025-07-21-CS231n_02/image-20250729101629408.png" alt="image-20250729101629408" style="zoom:50%;" />

때문에 보통은 가장자리에 0을 덧대어 크기를 유지하거나 혹은 다른 원하는 출력 크기를 얻을 수 있도록 조정해줍니다. 이를 **Padding**이라고 하는데요, 위와 같이 0을 덧대는 방식을 Zero-Padding이라고 합니다.



![6b81b78c-23ac-4c99-b4a8-fe586ad11ec5_960x540](/images/2025-07-21-CS231n_02/6b81b78c-23ac-4c99-b4a8-fe586ad11ec5_960x540.webp)

- <https://iaee.substack.com/p/yolo-intuitively-and-exhaustively>

이미지는 보통 여러 차원을 가지게 되는 경우가 많잖아요? 그 경우에는 필터도 입력 이미지와 동일한 차원을 가지게 됩니다. 그 결과물로는 가로 세로 길이는 조금 줄어든, 그리고 차원은 1인 이미지를 얻게 됩니다. 



![image-20250729102204211](/images/2025-07-21-CS231n_02/image-20250729102204211.png)

필터 개수당 1차원 출력을 얻게 되고, 필터의 개수만큼 출력 차원이 결정됩니다. 위 경우는 32 by 32 by 3의 이미지에 6개의 5 by 5 by 3 필터를 적용한 예시입니다.














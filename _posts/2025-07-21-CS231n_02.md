---
layout: single

title:  "CS231n: Backpropagation & NN(Neural Networks)"

categories: Computer_Vision

tag: [CV]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True


---





이 포스팅은 '**CS231n의 Lecture 04~07**'에 대한 내용을 담고 있습니다.



자료 출처

- <https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf>









# Neural Networks

지난 포스팅에 이어 Gradient Descent에 대해 좀 더 다뤄보고, 이어서 선형 분류기보다 더 진보한 방식이라 말할 수 있는 인공 신경망에 대해서 알아봅니다.







## Gradient Descent

**Gradient Descent**는 손실 함수를 낮출 때 쓰이는 아주 강력한 도구입니다.
쉽게 말해, 우리가 모델 안에 설정할 수 있는 **수많은 파라미터**(가중치, 편향 등)가 있잖아요? 이 파라미터들을 어떻게 움직이면 분류기의 성능이 좋아질지를 알려주는 것이 바로 **Gradient(기울기)**입니다.



![GD](/images/2025-07-21-CS231n_02/GD.gif)

위의 예시를 살펴봅시다. 빨간색 지점이 현재 모델의 파라미터 상태이고, 지도에서의 높이가 손실값이 됩니다. Gradient는 지금 이 방향으로 조금만 움직이면 더 낮은 손실 쪽으로 갈 수 있음을 가리켜주는 화살표 같은 역할을 합니다. 우리는 그 화살표 방향을 따라 조금씩 이동하면서 손실 값을 줄여나가는 거죠. 이 과정이 바로 **Gradient Descent**입니다.

 

![GDS](/images/2025-07-21-CS231n_02/GDS.gif)


$$
L(w) = \frac{1}{2}(w-3)^2
$$


이해를 위해 좀 더 단순한 예시를 만들어 봤습니다. $w$가 -5인 지점에서 출발하는 상황입니다. Gradient Descent는 말 그대로 **경사(기울기)를 따라 내려가는 알고리즘**이므로, 먼저 기울기를 구하고, **그 기울기의 반대 방향으로 이동**하게 됩니다.

그런데 왜 반대 방향일까요? 내리막길이라면 기울기가 음수로 나오고, 오르막길이라면 기울기가 양수로 나옵니다. 만약 기울기가 음수, 내리막길이면 $w$를 양수 방향으로 움직여야 내려가게 됩니다. 반대로 기울기가 양수, 오르막길이면 $w$를 음수 방향으로 움직여야 내려가게 됩니다. 즉, **기울기의 반대 방향으로 이동했을 때** 경사를 따라 내려가는 방향으로 변수가 움직입니다.



이 과정을 수식으로 쓰면,


$$
w \leftarrow w - \eta \nabla_wL(w)
$$


이렇게 표현됩니다.



위 식에서 $\eta$는 한 번 이동할 때의 기본 보폭을 의미하고요,  **Learning Rate(학습률)**이라고 부릅니다. 정리하면 기울기의 크기에 따라(가파르면 많이 움직이고, 완만하면 조금 움직이고), 그리고 정해진 기본 보폭에 곱해져서 최종적으로 움직이게될 거리가 결정됩니다.

이 Learning rate를 어떻게 설정하느냐가 Gradient Descent에서는 굉장히 중요합니다. 예를 들어, $\eta$를 너무 크게 잡으면 어떻게 될까요..? 위 예시의 **빨간 점**처럼, 내려가는 방향으로 움직였지만 보폭이 너무 커서 극소값을 지나쳐 버리고 오히려 더 높은 지점으로 올라가게 됩니다. 그러면 기울기가 더 커지고, 다시 큰 폭으로 움직여 더 멀리 올라가고… 이런 식으로 값이 점점 발산하게 됩니다. 반대로, $\eta$가 너무 작으면 어떻게 될까요..? **파란 점**처럼 아주 천천히 경사를 따라 내려가게 됩니다. 결국 극소값에는 도달하겠지만, 그 과정이 너무 느려져 학습 속도가 비효율적이 됩니다. 위 예시에서 **주황색과 초록색 점**처럼 적절한 Learning Rate를 선택하면 빠르고 안정적으로 극소값에 도달할 수 있습니다. 



남은 문제는 **그 화살표, 기울기를 어떻게 구할 것이냐**입니다. 기울기를 구하는 방식에 따라 두 가지로 나눌 수 있는데, 그 두 가지는 바로 **Numerical Gradient**와 **Analytic Gradient**입니다.



$$
\frac{df(x)}{dx} = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
$$



**Numerical Gradient**는 말 그대로, 실제로 파라미터를 아주 조금 바꿔보고 손실 값이 얼마나 변했는지를 직접 계산해서 기울기를 구하는 방식입니다. 위 수식은 우리가 잘 알고있는 미분의 정의인데 이거를 그대로 적용하는 거죠. 다만, 수학적으로는 $h$를 0에 한없이 가깝게 보내야 정확한 기울기 값이 나오지만, 실제 계산에서는 그렇게 하기가 어렵습니다. 대신 컴퓨터가 다룰 수 있는 적당히 작은 소수값을 $h$로 사용하게 되거든요? 이 과정에서 작지 않은 오차가 발생하게 됩니다. 그래서 Numerical Gradient는 직관적으로 이해하기 쉽다는 장점만 있을 뿐, 정확성 면에서는 한계가 있습니다. 

게다가 이 방식은 너무 느리다는 단점도 있어서 Numerical Gradient는 실제 학습에서는 거의 쓰이지 않고, 주로 **Analytic Gradient 방식의 기울기 계산이 맞는지 디버깅**할 때만 사용됩니다.

**Analytic Gradient**는 반대로, 각 파라미터에 대해 **직접 수식을 세워서** 기울기를 구하는 방식입니다. 고등학교 때 함수의 도함수를 직접 구했던 것처럼, 손실 함수를 각 파라미터에 대해 편미분해서 얻는 값입니다. 원리적으로 정확하고 빠르지만, 파라미터가 많으면  이걸 다 어떻게 미분하지..? 싶은 생각이 들 수 있죠. 하지만 다행히도, 우리는 이 계산을 **Backpropagation**이라는 기법을 사용해 단계적으로, 효율적으로 해낼 수 있습니다. 이 Backpropagation이라는 기법이 어떻게 동작하는지 바로 이어서 다루겠습니다.





### 간단한 모델에서의 Backpropagation


$$
f(x, y, z) = (ax + by) \times cz
$$

간단한 예시로 위와 같은 모델을 가정해 봅시다. 여기서 $x$, $y$, $z$는 입력 값이고, $a$, $b$, $c$는 우리가 조정할 수 있는 **파라미터**입니다. 손실 함수는 단순하게 모델의 예측 값과 정답(Ground Truth) 사이의 차이를 기준으로 하겠습니다. 



먼저 파라미터를 임의로 초기화해 봅시다.



$$
f(x, y, z) = (2x + 5y) \times 1z
$$



자 그리고 훈련 데이터 하나를 예시로 들어볼게요.


$$
\text{train example:}((x=1, y=2, z=1), \text{ground truth}=15)
$$



이 데이터를 모델에 넣어 계산해 보면,


$$
\hat y=(2×1+5×2)×(1×1)=(2+10)×1=12 
$$


예측 값 $\hat{y}$는 12이고, 실제 정답은 15이므로 오차는 3입니다. Gradient Descent로 파라미터를 업데이트하려면, 다음 세 개의 기울기가 필요합니다.


$$
\frac{\partial L}{\partial a}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial c}
$$


이 세 개의 값들을 구하기 위해 **Forward Propagation**과 **Backpropagation** 과정을 거치게 됩니다.



#### Forward Propagation

![FP](/images/2025-07-21-CS231n_02/FP.gif)

**Forward Propagation**은 입력에서 시작해 출력(예측 값)까지 차례대로 계산을 진행하는 과정입니다. 다만, 그냥 계산을 해 나가는 것이 아니라 계산 중간 결과들을 위와 같이 모두 저장해둡니다. 이를 **Memorization**이라고 하는데, 이 중간 값들이 나중에 편미분을 할 때 필요하거든요.. 이 작업이 Forward Propagation의 가장 중요한 부분입니다. 단순히 결과만 구하는 것이 아니라 나중에 실제로 Backpropgation 이라는 편미분(기울기) 값을 계산하는 과정에서 필요한 값들을 기록해 두는 것, 이게 .Forward Propagtion의 핵심입니다. 


$$
p = a x
$$

$$
q = b y
$$

$$
s = p + q
$$

$$
r = c z
$$

$$
t = s \times r
$$



이렇게 중간 변수들을 설정하고요, 실제로 계산을 해보면..


$$
p = 2 \times 1 = 2
$$

$$
q = 5 \times 2 = 10
$$

$$
s = 2 + 10 = 12
$$

$$
r = 1 \times 1 = 1
$$

$$
t = 12 \times 1 = 12
$$



즉, 


$$
p=2,\space q=10, \space s=12,\space r=1,\space t=12
$$


가 됩니다.



이렇게 저장해 둔 값들을 어떻게 각 파라미터의 변화량에 대한 손실 값의 변화량, 기울기를 계산하는지 Backpropagation 과정을 통해 살펴봅시다.



#### Backpropagation

Forward Propagation이 끝나면, 이제는 각 파라미터($a$, $b$, $c$)가 손실 값 $L$에 얼마나 영향을 주는지를 말하는 편미분 값, 기울기를 구해야 합니다. 이걸 구해주는 작업이 **Backpropagation** 입니다. **Backpropagation**은 손실 값과 각 파라미터 사이의 기울기를 구할 때 **Chain Rule**을 이용해 계산을 단계별로 전파하는 방법입니다.



> Q. Chain Rule이 뭔가요?
>
> Chain Rule은 어떤 값이 여러 단계의 계산을 거쳐서 결정될 때, 각 단계별 변화량을 모두 곱해 전체 변화량을 구하는 방법입니다. 예를 들어, $y = f(u)$이고 $u = g(x)$와 같이 $x$에서 $u$를 거쳐 $y$에 도달하게 되는 식의 흐름을 생각해봅시다. 이 예시에서 $x$에 대한 $y$의 변화량($\frac{dy}{dx}$)은  $\frac{dy}{du}$와 $\frac{du}{dx}$의 곱인 $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$를 통해 구해집니다. 이 예시보다 더 복잡한 식에서도 위의 예시에서 중간 변수로 $u$를 설정했듯이 더 많은 중간 변수를 통해 여러 간단한 블록으로 쪼갠 다음, 각 구간의 변화율을 곱하면 전체 변화율을 쉽게 구할 수 있습니다. Backpropagation에서는 이 원리를 활용해 손실 값과 식에 존재하는 모든 파라미터 사이의 기울기를 계산합니다.



아 그냥 귀찮게 중간 변수 설정 안 하고 한 번에 각 파라미터에 대해 편미분을 해버리면 되는 거 아닌가?.. 하는 생각이 들죠. 그런데요 그게 사실은 우리가 명시적으로 중간 변수를 쓰지 않을 뿐, 그 과정이 실제로는 중간 변수를 설정해가지고 Chain Rule 경로를 따라 계산하는거랑 동일합니다. 오히려 개별 파라미터에 대해 따로 편미분을 해버리면 Chain Rule 상에 등장하는 동일한 중간 변수에 대한 기울기를 여러 번 계산하는 꼴이 되어버리거든요? 이게 꽤나 큰 비효율을 야기합니다. 때문에 중복 계산을 하지 않도록 Backpropagation이라는 알고리즘을 사용합니다.



![BP](/images/2025-07-21-CS231n_02/BP-5180228.gif)

아까 위에서 $p = a x$, ... 이렇게 여러 중간변수들을 설정했습니다. 그리고 그 각각의 식들은 굉장히 간단한 형태(곱하거나 더하는게 끝)여서 눈으로 바로 계산할 수 있을 정도로 편미분이 쉽습니다. 그걸 다 적어주면요..


$$
\frac{\partial L}{\partial t} = -1
$$

$$
\frac{\partial t}{\partial s} = r
$$

$$
\frac{\partial t}{\partial r} = s
$$

$$
\frac{\partial s}{\partial p} = 1
$$

$$
\frac{\partial s}{\partial q} = 1
$$

$$
\frac{\partial p}{\partial a} = x
$$

$$
\frac{\partial q}{\partial b} = y
$$

$$
\frac{\partial r}{\partial c} = z
$$



이렇게 표현됩니다.



우변에 등장하는 $r$, $s$,... 이 변수들의 값($p=2,\space q=10, \space s=12,\space r=1,\space t=12$)을 Forward Propagation에서 다 구했었죠? $x$, $y$, $z$는 입력값으로 우리에게 주어진 값($x=1,\space y=2,\space z=1$)입니다. 이 친구들을 대입하면..



![DI](/images/2025-07-21-CS231n_02/DI.gif)



$$
\frac{\partial L}{\partial t} = -1
$$

$$
\frac{\partial t}{\partial s} = 1
$$

$$
\frac{\partial t}{\partial r} = 12
$$

$$
\frac{\partial s}{\partial p} = 1
$$

$$
\frac{\partial s}{\partial q} = 1
$$

$$
\frac{\partial p}{\partial a} = 1
$$

$$
\frac{\partial q}{\partial b} = 2
$$

$$
\frac{\partial r}{\partial c} = 1
$$



위와 같습니다. 이제 각각의 파라미터에 대한 기울기를 구하기 위해 Chain Rule을 사용합니다.



![EE](/images/2025-07-21-CS231n_02/EE.gif)



$$
\frac{\partial L}{\partial a}
= \frac{\partial L}{\partial t} \cdot
  \frac{\partial t}{\partial s} \cdot
  \frac{\partial s}{\partial p} \cdot
  \frac{\partial p}{\partial a}
= (-1) \cdot 1 \cdot 1 \cdot 1 = -1
$$

$$
\frac{\partial L}{\partial b}
= \frac{\partial L}{\partial t} \cdot
  \frac{\partial t}{\partial s} \cdot
  \frac{\partial s}{\partial q} \cdot
  \frac{\partial q}{\partial b}
= (-1) \cdot 1 \cdot 1 \cdot 2 = -2
$$

$$
\frac{\partial L}{\partial c}
= \frac{\partial L}{\partial t} \cdot
  \frac{\partial t}{\partial r} \cdot
  \frac{\partial r}{\partial c}
= (-1) \cdot 12 \cdot 1 = -12
$$



손실 값과 가까운 중간 변수부터 차례차례 기울기가 전파되어 나가는 모습을 볼 수 있죠. 이 과정에서 각 중간 변수에 한 번 씩만 편미분하기 때문에 각 파라미터에 대해 따로따로 편미분을 할 때보다 훨씬 더 효율적입니다.



위에서 다룬 예시는 아주 단순한 형태의 모델이었지만, 우리가 이전 포스팅에서 다뤘던 선형 분류 모델만 해도 훨씬 더 복잡한 구조를 가지고 있었습니다. 거기서는 가중치 행렬과 입력 벡터의 곱, 편향 벡터의 합과 같이 단순 스칼라 곱셈이 아니라 행렬 단위로 연산이 수행됩니다. 그런데요..


$$
ax + by = z
$$

$$
cx + dy = w
$$



라는 두 식이 있을 때, 이 친구들을..


$$
\begin{pmatrix} a & b \\ c & d \end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}
=
\begin{pmatrix} z \\ w \end{pmatrix}
$$


이렇게 선형 시스템으로 표현할 수 있었죠? 그 반대도 똑같습니다. **행렬로 표현된 식들도 결국은 각각의 스칼라 식들의 모음일 뿐**입니다. 스칼라 식들로 풀어 쓴 다음에 위에서 살펴본 예제와 똑같이 각 항목별로 Backpropagation이 수행하면 됩니다. 그러니까 행렬 형태를 쓴다고 해서 행렬인 경우에는 어떻게 하지?.. 라고 전혀 걱정할 필요가 없다는거죠.

실제 계산에서는 행렬 연산의 장점을 살려 한 번에 편미분을 하거나, 벡터·행렬 형태 그대로 미분 공식을 적용해 효율적으로 구현하지만, 그 계산의 근본 원리는 우리가 지금 본 단순한 스칼라 예제와 동일합니다.







## Neural Networks

지금까지 선형 분류 모델 구조, 그리고 해당 모델을 최적화시키기 위한 여러 기법들에 대해서 다루어 봤습니다. 그런데요 선형 분류 모델으로는 죽었다 깨어나도 절대 풀지 못하는 문제들이 있습니다. 가장 대표적으로는 **XOR** 분류 문제가 있습니다.



```python
import numpy as np
import matplotlib.pyplot as plt

def xor(a, b):
    """
    Compute the XOR operation for two binary inputs.
    Args:
        a (int): First binary input (0 or 1).
        b (int): Second binary input (0 or 1).
    Returns:
        int: Result of XOR operation (0 or 1).
    """
    return np.bitwise_xor(a, b)

# Example usage
inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
for a, b in inputs:
    print(f"XOR({a}, {b}) = {xor(a, b)}")

# XOR data
def generate_xor_data():
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XOR labels
    return X, y

# Linear classifier
class LinearClassifier:
    def __init__(self, learning_rate=0.1, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def forward(self, X):
        """
        Compute the linear output.
        """
        return np.dot(X, self.weights) + self.bias

    def train(self, X, y):
        """
        Train the linear classifier using gradient descent.
        """
        n_samples, n_features = X.shape
        self.weights = np.random.randn(n_features)
        self.bias = 0

        for _ in range(self.epochs):
            # Compute predictions
            linear_output = self.forward(X)
            predictions = 1 / (1 + np.exp(-linear_output))  # Sigmoid activation

            # Compute gradients
            errors = predictions - y
            grad_weights = np.dot(X.T, errors) / n_samples
            grad_bias = np.mean(errors)

            # Update parameters
            self.weights -= self.learning_rate * grad_weights
            self.bias -= self.learning_rate * grad_bias

    def predict(self, X):
        """
        Predict binary labels for input data.
        """
        linear_output = self.forward(X)
        predictions = 1 / (1 + np.exp(-linear_output))  # Sigmoid activation
        return (predictions >= 0.5).astype(int)

# Visualization
def visualize_decision_boundary(classifier, X, y):
    """
    Visualize the decision boundary of the trained classifier.
    """
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = classifier.predict(grid).reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.title("Decision Boundary of Linear Classifier on XOR Data")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Main execution
X, y = generate_xor_data()
classifier = LinearClassifier(learning_rate=0.1, epochs=1000)
classifier.train(X, y)
visualize_decision_boundary(classifier, X, y)
```



<img src="/images/2025-07-21-CS231n_02/image-20250728230415643.png" alt="image-20250728230415643" style="zoom:50%;" />

XOR 문제는 말 그대로 XOR로 위치하는 두 집단을 분류하는 문제입니다. 그리고 보시는 바와 같이, 그 두 집단을 하나의 직선으로 분류하는 것은 수학적으로 불가능합니다. 그렇다면 직선을 여러 개를 사용하거나 곡선을 사용하면 이런 문제들을 해결할 수 있을 것 같다는 생각이 듭니다. 그걸 해주는게 **Neural Networks** 입니다.

해결책으로 선형 레이어를 여러 개를 쌓으면 되지 않을까? 하는 생각이 듭니다. 그런데요 선형 계층을 무수히 많이 쌓아봤자 수학적으로는 하나의 선형 계측과 전혀 다르지 않습니다.


$$
\mathbf{h} = W_{1}\,\mathbf{x} + \mathbf{b}_{1}
$$

$$
\mathbf{y} = W_{2}\,\mathbf{h} + \mathbf{b}_{2}
$$



위 수식은 두 선형 계층이 $\mathbf h$ 라는 매개변수를 통해 이어진 모델을 나타냅니다. 두 식을 하나의 식으로 합쳐서 표현해볼게요.


$$
\mathbf{y}
= \bigl(W_{2} W_{1}\bigr)\,\mathbf{x}
  + \bigl(W_{2}\mathbf{b}_{1} + \mathbf{b}_{2}\bigr),
$$

$$
\mathbf{y}= W\,\mathbf{x} + \mathbf{b}
$$



두 수식을 합쳤더니 하나의 선형 계층으로 축약되어 버리는 결과를 확인할 수 있습니다. 이게 선형 계층을 아무리 많이 쌓아도 하나의 선형 계층과 전혀 다르지 않은 이유입니다. 이런 문제점을 해결하기 위해서 다음 선형 계층으로 넘어가기 전에 비선형 함수인 **Activation Function**을 넣어 줍니다. 


$$
\mathbf{z} = W_{1}\,\mathbf{x} + \mathbf{b}_{1}
$$

$$
\mathbf h = \phi(\mathbf z)
$$

$$
\mathbf{y} = W_{2}\,\mathbf{h} + \mathbf{b}_{2}
$$



이렇게 Activation Function을 사용해 선형 변환을 거친 출력을 휘어주면 여러 계층이 하나의 선형 함수로 더 이상 통합되지 못하고 각각이 결정 평면을 조절하는 역할을 수행하게 됩니다. 결과적으로는 복잡하게 구부러진 결정 평면을 만들 수 있게 되는 것입니다. 


$$
\mathbf{z}_1 = W_1\mathbf{x} + \mathbf{b}
$$

$$
\mathbf{h}_1 = \phi(\mathbf{z}_1)
$$



**Perceptron**은 '입력을 받아 선형 결합을 하고, 그 결과를 다시 비선형 함수로 가공해내는 최소 단위'를 의미합니다. 위에서는 행렬을 사용해서 한 번에 계산이 되기는 했지만, 사실 행렬을 통한 연산은 여러 개의 벡터 내적 연산을 함께 수행하는 것일 뿐이잖아요? 그 벡터 내적 연산이 되는 요소 하나하나를 Perceptron이라고 부릅니다. 예를 들어 입력 차원이 4차원이고 가중치 벡터가 3 by 4라면, 4차원 벡터끼리의 내적이 3번 수행되고 그 각각에 bias를 더한 다음 Activation Function을 가해 얻은 결과가 3차원 결과 벡터라는 거죠. 즉 3개의 Perceptron을 사용해 4차원 입력이 3차원 출력을 얻게 되었다고 말하는 것과 동일합니다. Perceptron은 기본적으로 모든 입력 값을 연산의 재료로 사용하기 때문에 Perceptron만으로 구성된 계층을 **FC(Fully-Connected)** Layer라고 부릅니다.



```python
import numpy as np
import matplotlib.pyplot as plt

# XOR data with additional slight noise for robustness
def generate_xor_data(noise=0.0):
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)
    if noise > 0:
        X += np.random.normal(scale=noise, size=X.shape)
    y = np.array([0, 1, 1, 0])  # XOR labels
    return X, y

# 2-layer neural network with Xavier initialization and Adam optimizer
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(a):
    return a * (1 - a)

class TwoLayerNN:
    def __init__(self, hidden_size=4, learning_rate=0.01, epochs=20000,
                 beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.hidden_size = hidden_size
        self.lr = learning_rate
        self.epochs = epochs
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

    def initialize_parameters(self, input_size):
        # Xavier/Glorot initialization
        limit_in = np.sqrt(6 / (input_size + self.hidden_size))
        self.W1 = np.random.uniform(-limit_in, limit_in, (input_size, self.hidden_size))
        self.b1 = np.zeros((1, self.hidden_size))
        limit_out = np.sqrt(6 / (self.hidden_size + 1))
        self.W2 = np.random.uniform(-limit_out, limit_out, (self.hidden_size, 1))
        self.b2 = np.zeros((1, 1))

        # Adam moment vectors
        self.mW1 = np.zeros_like(self.W1)
        self.vW1 = np.zeros_like(self.W1)
        self.mb1 = np.zeros_like(self.b1)
        self.vb1 = np.zeros_like(self.b1)
        self.mW2 = np.zeros_like(self.W2)
        self.vW2 = np.zeros_like(self.W2)
        self.mb2 = np.zeros_like(self.b2)
        self.vb2 = np.zeros_like(self.b2)

    def forward(self, X):
        self.X = X
        self.z1 = X @ self.W1 + self.b1
        self.a1 = np.tanh(self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = sigmoid(self.z2)
        return self.a2

    def compute_gradients(self, y):
        m = y.shape[0]
        y = y.reshape(-1, 1)
        # Output layer gradient for cross-entropy+sigmoid
        dz2 = (self.a2 - y) / m
        dW2 = self.a1.T @ dz2
        db2 = np.sum(dz2, axis=0, keepdims=True)
        # Hidden layer gradient
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (1 - np.tanh(self.z1)**2)
        dW1 = self.X.T @ dz1
        db1 = np.sum(dz1, axis=0, keepdims=True)
        return dW1, db1, dW2, db2

    def update_parameters_adam(self, dW1, db1, dW2, db2, t):
        # Update biased first moment estimate
        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dW1
        self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * db1
        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dW2
        self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * db2
        # Update biased second moment estimate
        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (dW1**2)
        self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (db1**2)
        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (dW2**2)
        self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (db2**2)
        # Compute bias-corrected moments
        mW1_hat = self.mW1 / (1 - self.beta1**t)
        mb1_hat = self.mb1 / (1 - self.beta1**t)
        mW2_hat = self.mW2 / (1 - self.beta1**t)
        mb2_hat = self.mb2 / (1 - self.beta1**t)
        vW1_hat = self.vW1 / (1 - self.beta2**t)
        vb1_hat = self.vb1 / (1 - self.beta2**t)
        vW2_hat = self.vW2 / (1 - self.beta2**t)
        vb2_hat = self.vb2 / (1 - self.beta2**t)
        # Update parameters
        self.W1 -= self.lr * mW1_hat / (np.sqrt(vW1_hat) + self.epsilon)
        self.b1 -= self.lr * mb1_hat / (np.sqrt(vb1_hat) + self.epsilon)
        self.W2 -= self.lr * mW2_hat / (np.sqrt(vW2_hat) + self.epsilon)
        self.b2 -= self.lr * mb2_hat / (np.sqrt(vb2_hat) + self.epsilon)

    def train(self, X, y):
        self.initialize_parameters(X.shape[1])
        for epoch in range(1, self.epochs + 1):
            output = self.forward(X)
            # Loss calculation (binary cross-entropy)
            eps = 1e-8
            loss = -np.mean(y.reshape(-1,1) * np.log(output + eps) + (1 - y.reshape(-1,1)) * np.log(1 - output + eps))
            # Gradients
            dW1, db1, dW2, db2 = self.compute_gradients(y)
            # Adam update
            self.update_parameters_adam(dW1, db1, dW2, db2, epoch)
            # Logging
            if epoch % 1000 == 0:
                print(f"Epoch {epoch:5d} — loss: {loss:.4f}")

    def predict(self, X):
        return (self.forward(X) >= 0.5).astype(int)

# Test cases
def test_xor():
    X, y = generate_xor_data(noise=0.0)
    model = TwoLayerNN(hidden_size=8, learning_rate=0.01, epochs=5000)
    model.train(X, y)
    preds = model.predict(X).flatten()
    print("XOR predictions:", preds)
    assert np.array_equal(preds, y), "Model failed to learn XOR"

if __name__ == "__main__":
    test_xor()
    print("All tests passed!")

# Visualization
X, y = generate_xor_data(noise=0.02)
model = TwoLayerNN(hidden_size=8, learning_rate=0.01, epochs=5000)
model.train(X, y)

# Plot decision boundary
def visualize_decision_boundary(classifier, X, y):
    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1
    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = classifier.predict(grid).reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)
    plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.title("Decision Boundary on XOR with Xavier + Adam")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

visualize_decision_boundary(model, X, y)

```

이 코드가 퍼셉트론을 2층으로 쌓아 만든 모델이구요, 아래 그림이 모델 학습 결과로 얻은 결과입니다.



<img src="/images/2025-07-21-CS231n_02/image-20250728234117419.png" alt="image-20250728234117419" style="zoom:50%;" />





### Convolution Neural Networks

일반적으로 MLP로 이미지를 분류하려면 일단 2차원 이미지를 일렬로 펴서 하나의 벡터로 만들어야 합니다. Perceptron은 모든 입력값과 연결되기 때문에 시작 정보의 공간 근접성이라는 특징을 무시합니다. 그러니까 굳이 볼 필요 없어 보이는 픽셀들을 함께 묶어 연산도 포함되는데, 이게 꽤나 많은 부분을 차지해서 특히 고해상도 이미지에서 크게 비효율적입니다. **CNN**(Convolution Neural Networks)는 주변 픽셀을 보자는 아이디어로 공간적 근접성을 고려할 수 있는 커널이라는 아이디어를 도입해 파라미터의 개수를 획기적으로 줄이면서도 더 높은 성능을 보여줍니다.



![cf](/images/2025-07-21-CS231n_02/cf.gif)



위와 같이 7  by 7 이미지가 있으면 저렇게 3 by 3의 Filter를 이동시키면서 겹치는 부분의 픽셀들을 모두 곱해 하나의 스칼라 값을 반환합니다. 위 그림에서는 가장 윗줄에 대해서만 이동시키면서 계산을 하고 있는데 다른 줄에 대해서도 동일한 연산을 수행하게 됩니다. 필터는 이미지를 벗어나지는 않기 때문에 CNN 연산을 수행하게 되면 원래 이미지보다 같거나 작은 크기의 출력을 얻습니다.



![cf2](/images/2025-07-21-CS231n_02/cf2.gif)

필터를 움직이는 보폭은 반드시 한 칸으로 정해진 것은 아니어서 이렇게 더 널찍이 이동할 수도 있습니다. 이 경우 출력의 크기가 당연히 더 줄어들게 되겠죠.



<img src="/images/2025-07-21-CS231n_02/image-20250729101629408.png" alt="image-20250729101629408" style="zoom:50%;" />

때문에 보통은 가장자리에 0을 덧대어 크기를 유지하거나 혹은 다른 원하는 출력 크기를 얻을 수 있도록 조정해줍니다. 이를 **Padding**이라고 하는데요, 위와 같이 0을 덧대는 방식을 Zero-Padding이라고 합니다.



![6b81b78c-23ac-4c99-b4a8-fe586ad11ec5_960x540](/images/2025-07-21-CS231n_02/6b81b78c-23ac-4c99-b4a8-fe586ad11ec5_960x540.webp)

- <https://iaee.substack.com/p/yolo-intuitively-and-exhaustively>

이미지는 보통 여러 차원을 가지게 되는 경우가 많잖아요? 그 경우에는 필터도 입력 이미지와 동일한 차원을 가집니다. 그 결과물로는 가로 세로 길이는 조금 줄어든, 그리고 차원은 1인 이미지를 얻게 됩니다. 



![image-20250729102204211](/images/2025-07-21-CS231n_02/image-20250729102204211.png)

필터 개수당 1차원 출력을 얻게 되고, 필터의 개수만큼 출력 차원이 결정됩니다. 위 경우는 32 by 32 by 3의 이미지에 6개의 5 by 5 by 3 필터를 적용한 예시입니다.



```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# 디바이스 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 데이터 전처리
mean = (0.4914, 0.4822, 0.4465)
std  = (0.2023, 0.1994, 0.2010)

transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

# 한 번만 다운로드하도록 플래그 설정
download_flag = not os.path.exists('./data/cifar-10-batches-py')

train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=download_flag, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=download_flag, transform=transform_test)

train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(
    test_dataset, batch_size=100, shuffle=False, num_workers=2)

# 모델 정의
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, 10)
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = self.relu(self.conv3(x))
        x = self.pool(x)
        x = x.view(-1, 128 * 4 * 4)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

model = SimpleCNN().to(device)

# 손실 함수 & 옵티마이저
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 함수
def train(epoch):
    model.train()
    running_loss = 0.0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if (batch_idx + 1) % 100 == 0:
            print(f'Epoch {epoch}, Step {batch_idx+1}, Loss: {running_loss/100:.4f}')
            running_loss = 0.0

# 평가 함수
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    print(f'Test Accuracy: {100.*correct/total:.2f}%')

# 메인
if __name__ == '__main__':
    num_epochs = 10
    for epoch in range(1, num_epochs+1):
        train(epoch)
        test()

```



<img src="/images/2025-07-21-CS231n_02/image-20250810124346117.png" alt="image-20250810124346117" style="zoom:50%;" />

위  코드는 CNN 레이어를 3층 쌓고, 뒤로 FC 레이어를 2층 쌓아 만든 모델입니다. 돌리시기 전에 pytorch 설치만 해주면 됩니다. 학습을 시켜보면 75% 정도의 테스트 성능이 나옵니다.










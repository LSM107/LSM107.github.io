---
layout: single

title:  "CS231n: Introduction to CNN for Visual Recognition"

categories: Computer_Vision

tag: [CV]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True


---





이 포스팅은 '**CS231n의 Lecture 01~03**'에 대한 내용을 담고 있습니다.



자료 출처

- <https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf>
- <https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf>









# Neural Networks

지난 포스팅에 이어 Gradient Descent에 대해 좀 더 다뤄보고, 이어서 선형 분류기보다 더 진보한 방식이라 말할 수 있는 인공 신경망에 대해서 알아봅니다.







## Gradient Descent

![GD](/images/2025-07-21-CS231n_02/GD.gif)

앞선 포스팅에서 다룬 Gradient Descent는 손실함수의 값을 낮출 때 사용되는 정말 강력한 도구입니다. 용어 그대로 Gradient를 따라 파라미터 값을 바꾸면서 점점 낮은 손실 값을 찾아가는 방법이기 때문에 파라미터와 손실 값 사이의 Gradient를 알고 있어야 사용할 수 있습니다.

Gradient를 구하는 방식에 따라 **Numerical Gradient**와 **Analytic Gradient**로 구분해 부릅니다. 


$$
\frac{df(x)}{dx} = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
$$


**Numerical Gradient**는 실제로 파라미터에 미소 변화를 주었을 때 손실 값의 변화를 측정하고 이들을 나누어 기울기를 계산합니다. 우리가 고등학교 때 배운 위의 미분 정의를 나타내는 식을 직접적으로 사용하는 것이죠. 이때 $h$는 무한히 작은 값일수록 더 정확해지지만 현실적으로는 적당히 아주 작은 숫자를 사용해 기울기를 구하게 됩니다. 그런데 이 방법을 통해 기울기를 구하려면 각 파라미터를 조금씩 바꿀 때마다 바뀐 손실함수 값을 알아야 하거든요? 그 말은 파라미터 개수만큼 evaluation을 수행해야 한다는 말이랑 같은데요, 당연히 시간이 엄청나게 많이 소요됩니다. 심지어는 그렇게 구한 기울기를 그리 정확하지도 않습니다. 이러한 문제점들로 인해 Numerical Gradient는 직관적이긴 하지만 잘 사용되지 않고 Analytic Gradient의 디버깅 용도로만 종종 사용됩니다.

**Analytic Gradient**는 각 변수에 대한 편미분 식을 직접 구합니다. 마치 고등학교에서 어떤 식에 대해서 미분을 할 때 도함수를 계산하잖아요? 그거랑 동일합니다. 그렇게 구한 편미분 기울기는 원리적으로 정확한 값입니다. 그런데 이걸로 구하려면 모든 파라미터에 대해서 편미분을 해야한다는 얘기 같아서 너무 복잡할 것 같은데, 실제로는 바로 아래에서 설명할 Backpropagation 기법을 통해 단계적으로, 그리고 효율적으로 구해집니다.





### Backpropagation


$$
f(x, y, z) = (ax + by) \times cz
$$


위의 모델을 가정해봅시다. 이 식에서 $x$, $y$, $z$는 입력 값이구요, $a$, $b$, $c$가 우리가 모델에서 조정할 수 있는 파라미터입니다. 손실함수는 단순하게 정답과 모델의 예측 값 사이의 차로 정의하겠습니다. 처음에 파라미터를 아래와 같이 랜덤으로 설정해보고, 모델을 평가하기 위한 하나의 훈련 데이터를 생각해볼게요.


$$
f(x, y, z) = (2x + 5y) \times 1z
$$

$$
\text{train example:}((x=1, y=2, z=1), \text{ground true}=15)
$$



훈련 데이터를 모델에 넣어보면 예측 값이 12이 나오고, 실제 Ground True 값은 15니까 손실 값은 3가 됩니다. Gradient Descent를 통해 업데이트하기 위해서는 $a$, $b$, $c$와 모델의 손실 값 사이의 기울기를 알아야 합니다.


$$
\frac{\partial L}{\partial a}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial c}
$$


세 개의 파라미터의 기울기가 Gradient Descent를 할 때 필요한 것들이고, 이 친구들은 Forward Propagation과 Backprogation을 통해 구해집니다.



![BP1](/images/2025-07-21-CS231n_02/BP1.gif)

먼저 **Forward Propagation**을 수행하는데, 예측 값을 구하기 위해 모델의 구조에 따라 순차적으로 계산해 나아가는 과정을 의미합니다. 그 과정 사이사이의 중간 계산 결과를  중간변수로 모두 저장해둡니다. 이어서 바로 **Backpropagation**을 실행합니다. Backpropagation의 핵심은 Chain Rule을 통한 기울기 전파입니다. 파라미터와 손실 값 사이의 기울기를 바로 계산하기는 복잡하지만, 중간변수 사이의 기울기를 구하는 과정은 굉장히 간단합니다(더하기와 곱하기 단위로 중간변수를 지정할 것이기 때문에). 그리고  매개변수와 손실 값 사이의 있는 중간변수들간의 기울기들을 Chain Rule을 통해 모조리 다 곱하면 결과적으로 손실 값과 파라미터 사이의 기울기를 구할 수 있게 됩니다. 

딱 봐도 동적 프로그래밍으로 구현하기 쉬운 구조로 보입니다. 실제로도 동적 프로그래밍을 사용해서 굉장히 효율적으로 모든 파라미터에 대한 기울기를 구하게 됩니다.


































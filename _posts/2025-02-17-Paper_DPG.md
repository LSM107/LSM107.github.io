---
layout: single

title:  "Deterministic Policy Gradient Algorithms(DPG)"

categories: Paper

tag: [Policy Gradient, Reinforcement Learning]

typora-root-url: ../

toc: true

author_profile: false

sidebar:
    nav: "docs"

# search: false
use_math: true
published: True

---



**글에 들어가기 앞서...**

이 포스팅은 '**Deterministic Policy Gradient Algorithms(DPG)**'에 대한 내용을 담고 있습니다.



자료 출처: <https://proceedings.mlr.press/v32/silver14.pdf>, <http://proceedings.mlr.press/v32/silver14-supp.pdf>, <https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf>









# Deterministic Policy Gradient Algorithms(DPG)

**Deterministic Policy Gradient(DPG, 결정론적 정책 경사 알고리즘)**는 강화학습 이론에서 다루는 정말 중요한 알고리즘 중 하나입니다. 결정론적 정책 경사 알고리즘 이전에는 확률론적 정채 경사 알고리즘들(예: REINFORCE)만이 존재했는데, 알고리즘의 구조상 연속적인 행동 공간을 가지는 환경에서 사용하기가 거의 불가능합니다.


$$
\begin{bmatrix}
a_1\\
a_2\\
a_3\\
\vdots \\
a_\infty
\end{bmatrix}
=
\begin{bmatrix}
\pi(a_1|s_t)\\
\pi(a_2|s_t)\\
\pi(a_3|s_t)\\
\vdots \\
\pi(a_\infty|s_t)
\end{bmatrix}
$$

확률론적 정책 경사 알고리즘에서는 각 행동별로 선택이 될 확률이 계산됩니다. 연속적인 행동 공간에서는 무한히 많은 행동이 존재하고, 이 모든 행동들에 대해 정책 확률을 부여하는 일은 매우 어렵습니다. 사실 연속적인 공간이 아니라, 다양한 액추에이터를 가지는 경우에서조차 알고리즘이 잘 동작하지 못합니다. 예를 들어 6축 로봇팔을 제어한다고 할 때, 각 모터의 범위를 0도부터 90도, 그리고 1도씩 discrete하게 행동공간을 정의해도 가능한 행동의 가짓 수가 무려 531,441,000,000개나 존재합니다. 때문에 이렇게 넓은 행동 공간을 가지는 경우에는 각 행동마다 선택할 확률을 부여하는 것이 아니라, 상태가 주어졌을 때, 다음 행동을 결정론적으로 선택하는 정책이 훨씬 더 유용합니다.


$$
\mu_\theta(s) = a
$$


생각해보면, 위와 같이 결정론적으로 정책 함수를 디자인하는게 떠올리기 어려운, 아주 새로운 아이디어는 아닌 것처럼 느껴집니다. 그렇다면 도대체 왜 2014년이 되어서야 제안이 되었고, 강화학습의 연속적인 행동 환경 문제에 있어서 이렇게까지 중요한 논문으로 다뤄지는걸까요? 

확률론적 정책 경사 알고리즘만이 있었을 당시에 사람들은 **모델이 없는 결정론적 정책 경사 알고리즘은 존재하지 않는다고 생각했습니다.** 그 이유는 확률론적 정책 경사 알고리즘에서 사용되는 **정책 경사 정리(Policy Gradient Theorem)**(*Sutton. 1999*)을 결정론적 정책을 사용하는 경우에는 사용할 수 없다는 점에 있습니다. **이 논문에서는 새로운 접근 방법을 통해 정책 경사 정리가 결정론적 정책 경사 알고리즘에서도 존재함을 보입니다.** 







## Introduction

먼저 당시 사람들이 왜 모델이 없는 결정론적 정책 경사 알고리즘에서는 정책 경사 정리가 존재하지 않는다고 생각했는지 설명하겠습니다. 아래는 **정책 경사 정리의 증명**입니다(*Sutton. 1999*). 





### Policy Gradient Theorem

에이전트에게 가이드가 되는 목표함수를 설정하는 방법에는 크게 두 가지가 있습니다. 하나는 모든 상태에서 받을 보상들에 대한 평균으로 설정하는 것이고, 다른 하나는 특정 상태에서 받을 보상으로 설정하는 방법입니다. Sutton의 정책 경사 정리에서는 두 가지 경우에서 모두 동일한 형태의 목표 기울기가 나타남을 보입니다.


$$
\rho(\pi) = \lim_{n\rightarrow\infty}\frac{1}{n}E[r_1 + r_2 + \cdots + r_n|\pi] = \sum_sd^\pi(s)\sum_a\pi(s, a)R_s^a
$$

$$
Q^\pi(s, a) = \sum_{t=1}^\infty E[r_t - \rho(\pi) | s_0 = s, a_0 = a, \pi]
$$



위는 모든 상태에서 받을 보상에 대한 평균의 공식과 이 경우 사용되는 상태-가치 함수 식입니다. 이어서 위의 목표함수에 대한 기울기를 유도합니다.



$$
V^\pi(s) = \sum_a\pi(s, a)Q^\pi(s, a)
$$

 양 변을 파라미터로 미분합니다.

 
$$
\frac{\partial V^\pi(s)}{\partial \theta} = \frac{\partial }{\partial \theta}\sum_a\pi(s, a)Q^\pi(s, a)
$$

$$
= \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)\frac{\partial }{\partial \theta}Q^\pi(s, a)] \text{ (곱미분)}
$$

$$
= \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)\frac{\partial }{\partial \theta}[R_s^a  + \sum_{s'}P_{ss'}^aV^\pi(s')- \rho(\pi)]] \text{ (상태-행동 가치 함수 전개)}
$$

$$
= \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[ - \frac{\partial }{\partial \theta}\rho(\pi) + \sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]
$$

$$
= \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]- \sum_a\pi(s, a)\frac{\partial }{\partial \theta}\rho(\pi)
$$

$$
= \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]- \frac{\partial }{\partial \theta}\rho(\pi)
$$

정리한 꼴을 다시 정리합니다. 

 

$$
\frac{\partial V^\pi(s)}{\partial \theta} = \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]- \frac{\partial }{\partial \theta}\rho(\pi)
$$

$$
\frac{\partial }{\partial \theta}\rho(\pi) = \sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]- \frac{\partial V^\pi(s)}{\partial \theta}
$$

$$
\sum_sd^\pi(s)\frac{\partial }{\partial \theta}\rho(\pi) = \sum_sd^\pi(s)[\sum_a[\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) + \pi(s, a)[\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')]]- \frac{\partial V^\pi(s)}{\partial \theta}]
$$

 

전개하면... 

$$
\sum_sd^\pi(s)\frac{\partial }{\partial \theta}\rho(\pi) 
= 

\sum_sd^\pi(s)\sum_a\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) 
+ \sum_sd^\pi(s)\sum_a\pi(s, a)\sum_{s'}P_{ss'}^a\frac{\partial }{\partial \theta}V^\pi(s')
$$

$$
- \sum_sd^\pi(s)\frac{\partial V^\pi(s)}{\partial \theta}
$$

 

 

$$
\sum_sd^\pi(s)\frac{\partial }{\partial \theta}\rho(\pi) 
= 

\sum_sd^\pi(s)\sum_a\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a) 
+\sum_{s'}d^\pi(s')\frac{\partial }{\partial \theta}V^\pi(s')
$$

$$
- \sum_sd^\pi(s)\frac{\partial V^\pi(s)}{\partial \theta}
$$

동일한 항을 제거하면...

 

$$
\frac{\partial\rho  }{\partial \theta}
= 

\sum_sd^\pi(s)\sum_a\frac{\partial \pi(s, a)}{\partial \theta}Q^\pi(s, a)
$$



 

마지막 식이 정책 경사 정리의 최종적인 꼴이 됩니다. 원문의 부록에서 두 번째 목표함수 설정에서도 동일한 꼴로 구해짐을 유도하는 과정을 확인할 수 있습니다. 





### 결정론적 정책에서의 Policy Gradient Theorem

원문에서 위와 같이 전개를 할 때, 파라미터화된 정책을 파라미터에 대해 미분할 수 있음을 가정합니다. 그런데, 결정론적 정책(Deterministic Policy)의 경우 정책에 대한 미분을 구하는게 불가능합니다. 정책 분포는 확률 밀도 함수이기 때문에, 일반적으로 결정론적 정책과 같이 discrete한 정책을 나타내기 어렵지만, 디렉 델타 함수를 사용한다면 표현이 불가능하지는 않습니다.


$$
\pi(a|s) = \delta(a - \mu_\theta(s))
$$


그런데 위와 같은 임펄스 함수는 애당초 미분이 불가능하기 때문에 Sutton의 정책 경사 정리를 사용할 수 없습니다. 다시 돌아가서 목표함수의 꼴을 다른 형태로 살펴보겠습니다.


$$
J(\tau) = E_{\sim\pi}[r(\tau)]
$$

$$
J(\tau) = \int \pi(\tau) \cdot r(\tau)d\tau
$$

$$
\nabla_\theta J(\tau) = \int \nabla_\theta\pi(\tau) \cdot r(\tau)d\tau
$$



위 식의 값을 구하기 위해서는 정책을 파라미터로 미분한 값을 구해야 합니다(확률론적 정책 경사 알고리즘에서는 로그 트릭을 사용해 꼴을 변경하지만, 결정론적 정책의 경우 로그 정책의 치역이 정의되지 않기 때문에 사용할 수 없습니다).



$$
\nabla_\theta\pi(\tau) = \nabla_\theta(p(s_0^\tau) \times \prod_{t=1}^{T}p(S_{t+1}|S_t, \mu(S_t)))
$$

$$
= \nabla_\theta p(s_0^\tau) \times \prod_{t=1}^{T}p(S_{t+1}|S_t, \mu(S_t)) +  p(s_0^\tau) \times \prod_{t=1}^{T}\nabla_\theta p(S_{t+1}|S_t, \mu(S_t))
$$

$$
\nabla_\theta p(S_{t+1}|S_t, \mu(S_t)) = \nabla_\mu p(S_{t+1}|S_t, \mu) \times \nabla_\theta \mu(S_t)
$$





정리하면, 상태 전이 확률의 미분값을 알고있어야 목표함수에 대한 기울기를 구할 수 있는데, 환경에 대한 모델이 있을 때에만 이 값을 구할 수 있습니다. 이러한 이유로 모델이 없는 경우의 결정론적 경사 하강 알고리즘(Model-Free Deterministic Policy Gradient)는 존재하지 않는다고 생각되었습니다.







## Deterministic Policy Gradient Theorem

아래는 논문의 Supplementary Material에서 보인 결정론적 정책 경사 정리의 유도 과정입니다.


$$
\nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))
$$

$$
= \nabla_\theta(r(s, \mu_\theta(s)) + \int_S\gamma p(s'|s, \mu_\theta(s))V^{\mu_\theta}(s')ds')
$$

$$
= \nabla_\theta \mu_\theta(s) \nabla_ar(s, a)|_{a=\mu_\theta(s)} + \nabla_\theta\int_S\gamma p(s'|s, \mu_\theta(s))V^{\mu_\theta}(s')ds' \text{ (합성 함수 미분)}
$$

$$
= \nabla_\theta \mu_\theta(s) \nabla_ar(s, a)|_{a=\mu_\theta(s)} 
+ \int_S\gamma( \nabla_\theta p(s'|s, \mu_\theta(s))V^{\mu_\theta}(s') 
+  p(s'|s, \mu_\theta(s))\space \nabla_\theta V^{\mu_\theta}(s'))ds'
\text{ (곱미분)}
$$

$$
= \nabla_\theta \mu_\theta(s) \nabla_ar(s, a)|_{a=\mu_\theta(s)} 
+ \int_S\gamma
( 
\nabla_\theta \mu_\theta(s) \nabla_ap(s'|s, a)|_{a = \mu_\theta(s)}       V^{\mu_\theta}(s') 
+  p(s'|s, \mu_\theta(s))\nabla_\theta V^{\mu_\theta}(s')
)ds'
\text{ (합성 함수 미분)}
$$



위의 식에서 동일한 인수를 가지고 있는 항끼리 묶으면...

 
$$
= \nabla_\theta \mu_\theta(s)\nabla_a(r(s, a) + \int_S \gamma p(s'| s, a)V^{\mu_\theta}(s')ds')|_{a=\mu_\theta(s)} + \int_S \gamma p(s'|s, \mu_\theta(s))\nabla_\theta V^{\mu_\theta}(s')ds'
$$

$$
= \nabla_\theta \mu_\theta(s)\nabla_aQ^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)} 
+ \int_S \gamma p(s'|s, \mu_\theta(s))\nabla_\theta V^{\mu_\theta}(s')ds' \text{ (상태-행동 함수 꼴 묶기)}
$$

$$
= \nabla_\theta \mu_\theta(s)\nabla_aQ^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)} 
+ \int_S \gamma p(s \rightarrow s', 1, \mu_\theta)\nabla_\theta V^{\mu_\theta}(s')ds'
$$



정리하면...


$$
\nabla_\theta V^{\mu_\theta}(s)= \nabla_\theta \mu_\theta(s)\nabla_aQ^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)} 
+ \int_S \gamma p(s \rightarrow s', 1, \mu_\theta)\nabla_\theta V^{\mu_\theta}(s')ds'
$$


위와 같이 상태 함수를 상태 함수를 사용해 표현되는 점화식을 얻을 수 있습니다. 점화식을 사용해 식을 무한히 전개해나면, 아래와 같이 간단한 꼴로 정리됩니다.


$$
\nabla_\theta V^{\mu_\theta}(s)= \int_S\sum_{t=0}^\infty \gamma^tp(s\rightarrow s', t, \mu_\theta)\nabla_\theta\mu_\theta(s') \nabla_aQ^{\mu_\theta}(s', a)|_{a=\mu_\theta(s')}ds'
$$


이제 위 식을 목적함수에 대입합니다.


$$
\nabla_\theta J(\mu_\theta) = \nabla_\theta\int_Sp_1(s)V^{\mu_\theta}(s)ds
$$

$$
= \int_Sp_1(s)\nabla_\theta V^{\mu_\theta}(s)ds
$$

$$
=\int_S\int_S\sum_{t=0}^\infty \gamma^tp_1(s)p(s\rightarrow s', t, \mu_\theta)\nabla_\theta\mu_\theta(s') \nabla_aQ^{\mu_\theta}(s', a)|_{a=\mu_\theta(s')}ds'ds
$$

$$
= \int_S \rho^{\mu_\theta}(s) \nabla_\theta\mu_\theta(s)\nabla_aQ^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)}ds
$$



위와 같이 결정론적 정책 경사 정리를 유도할 수 있습니다.









